{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import json\n",
    "from urllib.parse import unquote, urljoin, urlparse, urlsplit, urlunsplit\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "HTML_DIR = \"html\"\n",
    "OUT_URL_MAP = \"urlmap.txt\"\n",
    "OUT_WEB_GRAPH = \"webgraph.txt\"\n",
    "OUT_PAGERANK_SCORE = 'page_scores.txt'\n",
    "OUT_PAGERANK_MAPPER = 'page_rank_mapper.json'\n",
    "\n",
    "pages = []\n",
    "all_urls = []\n",
    "whitelist_file_types = ['html', 'htm']\n",
    "whitelist_domain = 'ku.ac.th'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dirpath, dirnames, filenames in os.walk(HTML_DIR):\n",
    "    if \"index.html\" in filenames:\n",
    "        url = \"https://\" + dirpath.replace(\"html/\", \"\") + \"/index.html\"\n",
    "        pages.append(\n",
    "            {\n",
    "                \"url\": url,\n",
    "                \"base_path\": \"https://\" + dirpath.replace(\"html/\", \"\"),\n",
    "                \"abs_path\": dirpath + \"/index.html\",\n",
    "            }\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'url': 'https://caras.arch.ku.ac.th/carasn/index.html',\n",
       " 'base_path': 'https://caras.arch.ku.ac.th/carasn',\n",
       " 'abs_path': 'html/caras.arch.ku.ac.th/carasn/index.html'}"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "pages[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_key = \"url\"\n",
    "pages = list({v[unique_key]: v for v in pages}.values())\n",
    "all_urls = [page[\"url\"] for page in pages]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_query_from_url(url):\n",
    "    parsed = urlparse(url)\n",
    "    return \"\".join([parsed.scheme, \"://\", parsed.netloc, parsed.path])\n",
    "\n",
    "\n",
    "def normalization_url(url, base_url):\n",
    "    try:\n",
    "        x = \"%s\" % url\n",
    "        # absolute\n",
    "        url = urljoin(base_url, url)\n",
    "\n",
    "        # remove # (self reference)\n",
    "        url = re.sub(r\"#.*\", \"\", url)\n",
    "\n",
    "        # parse to utf8\n",
    "        url = unquote(url)\n",
    "\n",
    "        # strip / (backslash)\n",
    "        url = url.strip(\"/\")\n",
    "\n",
    "        # remove query string\n",
    "        url = remove_query_from_url(url)\n",
    "\n",
    "        url = format_url(url)\n",
    "\n",
    "        return url\n",
    "    except Exception as e:\n",
    "        print(x, base_url)\n",
    "        #         print(x, url, base_url)\n",
    "        print(e)\n",
    "\n",
    "\n",
    "def format_url(url):\n",
    "    url = re.sub(r\"((?!:).)\\/\\/\", r\"\\g<1>/\", url)\n",
    "    parsed = urlparse(url)\n",
    "    hostname = parsed.hostname\n",
    "    url_path = parsed.path\n",
    "\n",
    "    filetype = re.match(r\".*\\.(.*)$\", url_path)\n",
    "    if filetype != None:\n",
    "        # urljoin 'http://localhost:8888/asdasd/htasd.html' => 'http://localhost:8888/asdasd/'\n",
    "        save_folder_path = hostname + \"/\".join(url_path.split(\"/\")[:-1])\n",
    "        save_filename = url.split(urljoin(url, \".\"))[1]\n",
    "    else:\n",
    "        save_folder_path = hostname + url_path\n",
    "        save_filename = \"index.html\"\n",
    "\n",
    "    save_folder_path = save_folder_path.strip(\"/\")\n",
    "    save_abs_path = \"https://\" + save_folder_path + \"/\" + save_filename\n",
    "\n",
    "    return save_abs_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filters_urls(urls, base_url):\n",
    "    filtered_urls = []\n",
    "    for url in urls:\n",
    "        parsed = urlparse(url)\n",
    "        url_path = parsed.path\n",
    "        hostname = parsed.hostname\n",
    "        # check domain allow only ku.ac.th\n",
    "        if not hostname or whitelist_domain not in hostname:\n",
    "            continue\n",
    "\n",
    "        # check filetype\n",
    "        filetype = re.match(r\".*\\.(.*)$\", url_path)\n",
    "\n",
    "        if not filetype:\n",
    "            filtered_urls.append(url)\n",
    "        elif filetype[1] in whitelist_file_types:\n",
    "            filtered_urls.append(url)\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    return filtered_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pages = ['http://https://www.ku.ac.th/wlh']\n",
    "\n",
    "for i in range(len(pages)):\n",
    "    page = pages[i]\n",
    "    url = page['url']\n",
    "    base_path = page['base_path']\n",
    "    abs_path = page['abs_path']\n",
    "    \n",
    "    with open(abs_path, 'r') as file:\n",
    "       raw_html = file.read()\n",
    "    \n",
    "    soup = BeautifulSoup(raw_html, 'html.parser')\n",
    "    href_list = []\n",
    "    for page in soup.find_all('a', href=True):\n",
    "        href_list.append(page.get('href'))\n",
    "    href_list = filters_urls(href_list, base_path)\n",
    "    href_list = [normalization_url(href, base_path) for href in href_list]\n",
    "    href_list = [href for href in href_list if href in all_urls]\n",
    "    href_list = list(set(href_list))\n",
    "    \n",
    "    adj_list = [str(all_urls.index(href)) for href in href_list if href in all_urls]\n",
    "    # eliminate self loop\n",
    "    adj_list = [adj for adj in adj_list if str(adj) != str(i)]\n",
    "    \n",
    "    with open(OUT_WEB_GRAPH, \"a\") as file:\n",
    "        file.write(','.join(adj_list) + '\\n')\n",
    "    \n",
    "    with open(OUT_URL_MAP, \"a\") as file:\n",
    "        file.write(url + '\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Page Rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges = []\n",
    "num_node = 0\n",
    "\n",
    "with open(OUT_WEB_GRAPH, \"r\") as file:\n",
    "    lines = file.readlines()\n",
    "    num_node = len(lines)\n",
    "    for i in range(len(lines)):\n",
    "        line = lines[i].strip().split(',')\n",
    "        for adj in line:\n",
    "            if(adj):\n",
    "                edges.append([i, int(adj), 1/len(line)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = np.zeros(shape=(num_node, num_node))\n",
    "\n",
    "for edge in edges:\n",
    "    p[(edge[0], edge[1])] = edge[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pagerank(p, alpha=0.85, max_iter=100, tol=1.0e-6):\n",
    "    initial_value = 1 / num_node\n",
    "    \n",
    "    # solve rank leak problem\n",
    "    for i in range(len(p)):\n",
    "        sum_row = sum(p[i])\n",
    "        if(sum_row == 0):\n",
    "            p[i] = np.full((num_node,), initial_value)\n",
    "    \n",
    "    r = np.full((num_node, 1), initial_value)\n",
    "    p_t = p.T\n",
    "    for i in range(max_iter):\n",
    "        prev_r = r.copy()\n",
    "        r = alpha * p_t.dot(prev_r) + (1 - alpha) * np.full((num_node, 1), initial_value)\n",
    "\n",
    "        err = sum(abs(r - prev_r))\n",
    "        if(err < tol):\n",
    "            return r\n",
    "\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pagerank_scores = pagerank(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "pagerank_scores = pagerank_scores.reshape(num_node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(OUT_PAGERANK_SCORE, \"w\") as file:\n",
    "    file.write('\\n'.join(([str(score) for score in pagerank_scores])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pagerank_mapper = {pages[i]['url']: pagerank_scores[i] for i in range(len(pages))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(OUT_PAGERANK_MAPPER, 'w') as file:\n",
    "    file.write(json.dumps(pagerank_mapper, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}